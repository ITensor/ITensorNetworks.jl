var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = ITensorNetworks","category":"page"},{"location":"#ITensorNetworks","page":"Home","title":"ITensorNetworks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for ITensorNetworks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [ITensorNetworks]","category":"page"},{"location":"#ITensorNetworks.CURRENT_PARTITIONING_BACKEND","page":"Home","title":"ITensorNetworks.CURRENT_PARTITIONING_BACKEND","text":"Current default graph partitioning backend\n\n\n\n\n\n","category":"constant"},{"location":"#ITensorNetworks.Backend","page":"Home","title":"ITensorNetworks.Backend","text":"Graph partitioning backend\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.ITensorNetwork","page":"Home","title":"ITensorNetworks.ITensorNetwork","text":"ITensorNetwork\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.ProjMPOApply","page":"Home","title":"ITensorNetworks.ProjMPOApply","text":"A ProjMPOApply represents the application of an MPO H onto an MPS psi0 but \"projected\" by the basis of a different MPS psi (which could be an approximation to H|psi>).\n\nAs an implementation of the AbstractProjMPO type, it supports multiple nsite values for one- and two-site algorithms.\n\n     *--*--*-      -*--*--*--*--*--* <psi|\n     |  |  |  |  |  |  |  |  |  |  |\n     h--h--h--h--h--h--h--h--h--h--h H  \n     |  |  |  |  |  |  |  |  |  |  |\n     o--o--o-      -o--o--o--o--o--o |psi0>\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.ProjMPS2","page":"Home","title":"ITensorNetworks.ProjMPS2","text":"Holds the following data where psi is the MPS being optimized and M is the  MPS held constant by the ProjMPS.\n\n     o--o--o--o--o--o--o--o--o--o--o <M|\n     |  |  |  |  |  |  |  |  |  |  |\n     *--*--*-      -*--*--*--*--*--* |psi>\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.ProjTTNO","page":"Home","title":"ITensorNetworks.ProjTTNO","text":"ProjTTNO\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.ProjTTNOSum","page":"Home","title":"ITensorNetworks.ProjTTNOSum","text":"ProjTTNOSum\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.TDVPInfo","page":"Home","title":"ITensorNetworks.TDVPInfo","text":"#fields\n\nmaxtruncerr::Float64: the maximum tuncation error\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.TTNO-Union{Tuple{V}, Tuple{ITensors.LazyApply.Applied{typeof(sum), Tuple{Array{ITensors.LazyApply.Applied{typeof(*), Tuple{C, ITensors.LazyApply.Prod{ITensors.Ops.Op}}, NamedTuple{(), Tuple{}}}, 1}}, NamedTuple{(), Tuple{}}} where C, IndsNetwork{V, <:ITensors.Index}}} where V","page":"Home","title":"ITensorNetworks.TTNO","text":"TTNO(os::OpSum, sites::IndsNetwork{<:Index}; kwargs...)\nTTNO(eltype::Type{<:Number}, os::OpSum, sites::IndsNetwork{<:Index}; kwargs...)\n\nConvert an OpSum object os to a TreeTensorNetworkOperator, with indices given by sites.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.TreeTensorNetworkOperator","page":"Home","title":"ITensorNetworks.TreeTensorNetworkOperator","text":"TreeTensorNetworkOperator <: AbstractITensorNetwork\n\nA finite size tree tensor network operator type.  Keeps track of the orthogonality center.\n\nFields\n\nitensor_network::ITensorNetwork{V}\northo_lims::Vector{V}: A vector of vertices defining the orthogonality limits.\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.TreeTensorNetworkState","page":"Home","title":"ITensorNetworks.TreeTensorNetworkState","text":"TreeTensorNetworkState{V} <: AbstractITensorNetwork{V}\n\nFields\n\nitensor_network::ITensorNetwork{V}\northo_lims::Vector{V}: A vector of vertices defining the orthogonality limits.\n\n\n\n\n\n","category":"type"},{"location":"#ITensorNetworks.contraction_sequence-Tuple{ITensors.Algorithm{:greedy}, Vector{ITensors.ITensor}}","page":"Home","title":"ITensorNetworks.contraction_sequence","text":"GreedyMethod(; method=MinSpaceOut(), nrepeat=10)\n\nThe fast but poor greedy optimizer. Input arguments are:\n\nmethod is MinSpaceDiff() or MinSpaceOut.\nMinSpaceOut choose one of the contraction that produces a minimum output tensor size,\nMinSpaceDiff choose one of the contraction that decrease the space most.\nnrepeat is the number of repeatition, returns the best contraction order.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.contraction_sequence-Tuple{ITensors.Algorithm{:kahypar_bipartite}, Any}","page":"Home","title":"ITensorNetworks.contraction_sequence","text":"KaHyParBipartite(; sc_target, imbalances=collect(0.0:0.005:0.8),\n                   max_group_size=40, greedy_config=GreedyMethod())\n\nOptimize the einsum code contraction order using the KaHyPar + Greedy approach. This program first recursively cuts the tensors into several groups using KaHyPar, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are:\n\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nimbalances is a KaHyPar parameter that controls the group sizes in hierarchical bipartition,\nmax_group_size is the maximum size that allowed to used greedy search,\ngreedy_config is a greedy optimizer.\n\nReferences\n\nHyper-optimized tensor network contraction\nSimulating the Sycamore quantum supremacy circuits\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.contraction_sequence-Tuple{ITensors.Algorithm{:sa_bipartite}, Any}","page":"Home","title":"ITensorNetworks.contraction_sequence","text":"SABipartite(; sc_target=25, ntrials=50, βs=0.1:0.2:15.0, niters=1000\n              max_group_size=40, greedy_config=GreedyMethod(), initializer=:random)\n\nOptimize the einsum code contraction order using the Simulated Annealing bipartition + Greedy approach. This program first recursively cuts the tensors into several groups using simulated annealing, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are:\n\nsize_dict, a dictionary that specifies leg dimensions,\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nmax_group_size is the maximum size that allowed to used greedy search,\nβs is a list of inverse temperature 1/T,\nniters is the number of iteration in each temperature,\nntrials is the number of repetition (with different random seeds),\ngreedy_config configures the greedy method,\ninitializer, the partition configuration initializer, one can choose :random or :greedy (slow but better).\n\nReferences\n\nHyper-optimized tensor network contraction\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.contraction_sequence-Tuple{ITensors.Algorithm{:tree_sa}, Any}","page":"Home","title":"ITensorNetworks.contraction_sequence","text":"TreeSA(; sc_target=20, βs=collect(0.01:0.05:15), ntrials=10, niters=50,\n         sc_weight=1.0, rw_weight=0.2, initializer=:greedy, greedy_config=GreedyMethod(; nrepeat=1))\n\nOptimize the einsum contraction pattern using the simulated annealing on tensor expression tree.\n\nsc_target is the target space complexity,\nntrials, βs and niters are annealing parameters, doing ntrials indepedent annealings, each has inverse tempteratures specified by βs, in each temperature, do niters updates of the tree.\nsc_weight is the relative importance factor of space complexity in the loss compared with the time complexity.\nrw_weight is the relative importance factor of memory read and write in the loss compared with the time complexity.\ninitializer specifies how to determine the initial configuration, it can be :greedy or :random. If it is using :greedy method to generate the initial configuration, it also uses two extra arguments greedy_method and greedy_nrepeat.\nnslices is the number of sliced legs, default is 0.\nfixed_slices is a vector of sliced legs, default is [].\n\nReferences\n\nRecursive Multi-Tensor Contraction for XEB Verification of Quantum Circuits\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.current_partitioning_backend-Tuple{}","page":"Home","title":"ITensorNetworks.current_partitioning_backend","text":"Get the graph partitioning backend\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.delta_network-Tuple{Type, IndsNetwork}","page":"Home","title":"ITensorNetworks.delta_network","text":"RETURN A TENSOR NETWORK WITH COPY TENSORS ON EACH VERTEX.  Note that passing a link_space will mean the indices of the resulting network don't match those of the input indsnetwork\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.find_subgraph-Tuple{Any, DataGraph}","page":"Home","title":"ITensorNetworks.find_subgraph","text":"Find the subgraph which contains the specified vertex.\n\nTODO: Rename something more general, like:\n\nfindfirstinvertex_data(item, graph::AbstractDataGraph)\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.findall_on_edges-Tuple{Function, DataGraphs.AbstractDataGraph}","page":"Home","title":"ITensorNetworks.findall_on_edges","text":"Find all edges e such that f(graph[e]) == true\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.findall_on_vertices-Tuple{Function, DataGraphs.AbstractDataGraph}","page":"Home","title":"ITensorNetworks.findall_on_vertices","text":"Find all vertices v such that f(graph[v]) == true\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.findfirst_on_edges-Tuple{Function, DataGraphs.AbstractDataGraph}","page":"Home","title":"ITensorNetworks.findfirst_on_edges","text":"Find the edge e such that f(graph[e]) == true\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.findfirst_on_vertices-Tuple{Function, DataGraphs.AbstractDataGraph}","page":"Home","title":"ITensorNetworks.findfirst_on_vertices","text":"Find the vertex v such that f(graph[v]) == true\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.finite_state_machine-Union{Tuple{V}, Tuple{C}, Tuple{ITensors.LazyApply.Applied{typeof(sum), Tuple{Array{ITensors.LazyApply.Applied{typeof(*), Tuple{C, ITensors.LazyApply.Prod{ITensors.Ops.Op}}, NamedTuple{(), Tuple{}}}, 1}}, NamedTuple{(), Tuple{}}}, IndsNetwork{V, <:ITensors.Index}, V}} where {C, V}","page":"Home","title":"ITensorNetworks.finite_state_machine","text":"finite_state_machine(os::OpSum{C}, sites::IndsNetwork{V,<:Index}, root_vertex::V) where {C,V}\n\nFinite state machine generator for ITensors.OpSum Hamiltonian defined on a tree graph. The site Index graph must be a tree graph, and the chosen root  must be a leaf vertex of this tree. Returns a DataGraph of SparseArrayKit.SparseArrays\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.fsmTTNO-Union{Tuple{V}, Tuple{C}, Tuple{ITensors.LazyApply.Applied{typeof(sum), Tuple{Array{ITensors.LazyApply.Applied{typeof(*), Tuple{C, ITensors.LazyApply.Prod{ITensors.Ops.Op}}, NamedTuple{(), Tuple{}}}, 1}}, NamedTuple{(), Tuple{}}}, IndsNetwork{V, <:ITensors.Index}, V}} where {C, V}","page":"Home","title":"ITensorNetworks.fsmTTNO","text":"fsmTTNO(os::OpSum{C}, sites::IndsNetwork{V,<:Index}, root_vertex::V, kwargs...) where {C,V}\n\nConstruct a dense TreeTensorNetworkOperator from sparse finite state machine represenatation, without compression.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.get_single_site_expec-Tuple{ITensorNetwork, DataGraph, ITensorNetwork, Any}","page":"Home","title":"ITensorNetworks.get_single_site_expec","text":"given two flat networks psi and psi0, calculate the ratio of their contraction centred on the the subgraph containing v. The message tensors should be formulated over psi Link indices between psi and psi0 should be consistent so the mts can be applied to both\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.get_two_site_expec-Tuple{ITensorNetwork, DataGraph, ITensorNetwork, Any, Any}","page":"Home","title":"ITensorNetworks.get_two_site_expec","text":"given two flat networks psi and psi0, calculate the ratio of their contraction centred on the subgraph(s) containing v1 and v2. The message tensors should be formulated over psi.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.heisenberg-Tuple{Graphs.AbstractGraph}","page":"Home","title":"ITensorNetworks.heisenberg","text":"Random field J1-J2 Heisenberg model on a general graph\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.heisenberg-Tuple{Integer}","page":"Home","title":"ITensorNetworks.heisenberg","text":"Random field J1-J2 Heisenberg model on a chain of length N\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.ising-Tuple{Graphs.AbstractGraph}","page":"Home","title":"ITensorNetworks.ising","text":"Next-to-nearest-neighbor Ising model (ZZX) on a general graph\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.ising-Tuple{Integer}","page":"Home","title":"ITensorNetworks.ising","text":"Next-to-nearest-neighbor Ising model (ZZX) on a chain of length N\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.ising_network-Tuple{Type, IndsNetwork, Number}","page":"Home","title":"ITensorNetworks.ising_network","text":"BUILD Z OF CLASSICAL ISING MODEL ON A GIVEN GRAPH AT INVERSE TEMP BETA H = -\\sum{(v,v') \\in edges}\\sigma^{z}{v}\\sigma^{z}_{v'} TAKE AS AN OPTIONAL ARGUMENT A LIST OF VERTICES OVER WHICH TO APPLY A SZ. THE RESULTANT NETWORK CAN THEN BE CONTRACTED AND DIVIDED BY THE ACTUAL PARTITION FUNCTION TO GET THAT OBSERVABLE INDSNETWORK IS ASSUMED TO BE BUILT FROM A GRAPH (NO SITE INDS) AND OF LINK SPACE 2\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.ising_network-Tuple{Type, NamedGraphs.NamedGraph, Number}","page":"Home","title":"ITensorNetworks.ising_network","text":"BUILD Z OF CLASSICAL ISING MODEL ON A GIVEN GRAPH AT INVERSE TEMP BETA H = -\\sum{(v,v') \\in edges}\\sigma^{z}{v}\\sigma^{z}_{v'} TAKE AS AN OPTIONAL ARGUMENT A LIST OF VERTICES OVER WHICH TO APPLY A SZ. THE RESULTANT NETWORK CAN THEN BE CONTRACTED AND DIVIDED BY THE ACTUAL PARTITION FUNCTION TO GET THAT OBSERVABLE\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.iterate_single_site_expec-Tuple{ITensorNetwork, DataGraph, Int64, ITensorNetwork, Any}","page":"Home","title":"ITensorNetworks.iterate_single_site_expec","text":"Starting with initial guess for messagetensors, monitor the convergence of an observable on a single site v (which is emedded in tnO)\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.iterate_two_site_expec-Tuple{ITensorNetwork, DataGraph, Int64, ITensorNetwork, Any, Any}","page":"Home","title":"ITensorNetworks.iterate_two_site_expec","text":"Starting with initial guess for messagetensors, monitor the convergence of an observable on a pair of sites v1 and v2 (which is emedded in tnO)\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.partition-Tuple{Graphs.AbstractGraph}","page":"Home","title":"ITensorNetworks.partition","text":"partition(g::AbstractGraph; npartitions::Integer, kwargs...)\npartition(g::AbstractGraph, subgraph_vertices)\n\nGiven a graph g, partition g into npartitions partitions or into partitions with nvertices_per_partition vertices per partition. The partitioning tries to keep all subgraphs the same size and minimize edges cut between them.\n\nAlternatively, specify a desired partitioning with a collection of sugraph vertices.\n\nReturns a data graph where each vertex contains the corresponding subgraph as vertex data. The edges indicates which subgraphs are connected, and the edge data stores a dictionary with two fields. The field :edges stores a list of the edges of the original graph that were connecting the two subgraphs, and :edge_data stores a dictionary mapping edges of the original graph to the data living on the edges of the original graph, if it existed.\n\nTherefore, one should be able to extract that data and recreate the original graph from the results of partition.\n\nA graph partitioning backend such as Metis or KaHyPar needs to be installed for this function to work if the subgraph vertices aren't specified explicitly.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.partition_vertices-Tuple{Graphs.AbstractGraph, Any}","page":"Home","title":"ITensorNetworks.partition_vertices","text":"partition_vertices(g::AbstractGraph, subgraph_vertices::Vector)\n\nGiven a graph (g) and groups of vertices defining subgraphs of that graph (subgraph_vertices), return a DataGraph storing the subgraph vertices on the vertices of the graph and with edges denoting which subgraphs of the original graph have edges connecting them, along with edge data storing the original edges that were connecting the subgraphs.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.partition_vertices-Tuple{Graphs.AbstractGraph}","page":"Home","title":"ITensorNetworks.partition_vertices","text":"partition_vertices(g::AbstractGraph; npartitions, nvertices_per_partition, kwargs...)\n\nGiven a graph g, partition the vertices of g into 'npartitions' partitions or into partitions with nvertices_per_partition vertices per partition. Try to keep all subgraphs the same size and minimise edges cut between them Returns a datagraph where each vertex contains the list of vertices involved in that subgraph. The edges state which subgraphs are connected. A graph partitioning backend such as Metis or KaHyPar needs to be installed for this function to work.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.randomITensorNetwork-Tuple{Type, IndsNetwork}","page":"Home","title":"ITensorNetworks.randomITensorNetwork","text":"Build an ITensor network on a graph specified by the inds network s. Bonddim is given by linkspace and entries are randomised (normal distribution, mean 0 std 1)\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.relabel_sites-Tuple{ITensors.LazyApply.Applied{typeof(sum), Tuple{Array{ITensors.LazyApply.Applied{typeof(*), Tuple{C, ITensors.LazyApply.Prod{ITensors.Ops.Op}}, NamedTuple{(), Tuple{}}}, 1}}, NamedTuple{(), Tuple{}}} where C, Dictionaries.AbstractDictionary}","page":"Home","title":"ITensorNetworks.relabel_sites","text":"Relabel sites in OpSum according to given site map\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.set_partitioning_backend!-Tuple{Union{Missing, String, ITensorNetworks.Backend}}","page":"Home","title":"ITensorNetworks.set_partitioning_backend!","text":"Set the graph partitioning backend\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.subgraphs-Tuple{Graphs.AbstractGraph, Any}","page":"Home","title":"ITensorNetworks.subgraphs","text":"subgraphs(g::AbstractGraph, subgraph_vertices)\n\nReturn a collection of subgraphs of g defined by the subgraph vertices subgraph_vertices.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.subgraphs-Tuple{Graphs.AbstractGraph}","page":"Home","title":"ITensorNetworks.subgraphs","text":"subgraphs(g::AbstractGraph; npartitions::Integer, kwargs...)\n\nGiven a graph g, partition g into npartitions partitions or into partitions with nvertices_per_partition vertices per partition, returning a list of subgraphs. Try to keep all subgraphs the same size and minimise edges cut between them. A graph partitioning backend such as Metis or KaHyPar needs to be installed for this function to work.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.svdTTNO-Union{Tuple{VT}, Tuple{C}, Tuple{ITensors.LazyApply.Applied{typeof(sum), Tuple{Array{ITensors.LazyApply.Applied{typeof(*), Tuple{C, ITensors.LazyApply.Prod{ITensors.Ops.Op}}, NamedTuple{(), Tuple{}}}, 1}}, NamedTuple{(), Tuple{}}}, IndsNetwork{VT, <:ITensors.Index}, VT}} where {C, VT}","page":"Home","title":"ITensorNetworks.svdTTNO","text":"svdTTNO(os::OpSum{C}, sites::IndsNetwork{V<:Index}, root_vertex::V, kwargs...) where {C,V}\n\nConstruct a dense TreeTensorNetworkOperator from a symbolic OpSum representation of a Hamiltonian, compressing shared interaction channels.\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.tdvp-Tuple{Any, ITensors.MPO, Number, ITensors.MPS}","page":"Home","title":"ITensorNetworks.tdvp","text":"tdvp(H::MPO,psi0::MPS,t::Number; kwargs...)\ntdvp(H::MPO,psi0::MPS,t::Number; kwargs...)\n\nUse the time dependent variational principle (TDVP) algorithm to compute exp(t*H)*psi0 using an efficient algorithm based on alternating optimization of the MPS tensors and local Krylov exponentiation of H.\n\nReturns:\n\npsi::MPS - time-evolved MPS\n\nOptional keyword arguments:\n\noutputlevel::Int = 1 - larger outputlevel values resulting in printing more information and 0 means no output\nobserver - object implementing the Observer interface which can perform measurements and stop early\nwrite_when_maxdim_exceeds::Int - when the allowed maxdim exceeds this value, begin saving tensors to disk to free memory in large calculations\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.tdvp-Tuple{Any, Vector{ITensors.MPO}, Number, ITensors.MPS}","page":"Home","title":"ITensorNetworks.tdvp","text":"tdvp(Hs::Vector{MPO},psi0::MPS,t::Number; kwargs...)\ntdvp(Hs::Vector{MPO},psi0::MPS,t::Number, sweeps::Sweeps; kwargs...)\n\nUse the time dependent variational principle (TDVP) algorithm to compute exp(t*H)*psi0 using an efficient algorithm based on alternating optimization of the MPS tensors and local Krylov exponentiation of H.\n\nThis version of tdvp accepts a representation of H as a Vector of MPOs, Hs = [H1,H2,H3,...] such that H is defined as H = H1+H2+H3+... Note that this sum of MPOs is not actually computed; rather the set of MPOs [H1,H2,H3,..] is efficiently looped over at  each step of the algorithm when optimizing the MPS.\n\nReturns:\n\npsi::MPS - time-evolved MPS\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.to_vec-Tuple{Any}","page":"Home","title":"ITensorNetworks.to_vec","text":"to_vec(x)\n\nTransform x into a Vector, and return the vector, and a closure which inverts the transformation.\n\nModeled after FiniteDifferences.to_vec:\n\nhttps://github.com/JuliaDiff/FiniteDifferences.jl/blob/main/src/to_vec.jl\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.two_site_rdm_bp-Tuple{ITensorNetwork, ITensorNetwork, DataGraph, Any, Any, IndsNetwork, DataGraph}","page":"Home","title":"ITensorNetworks.two_site_rdm_bp","text":"Get twositerdm using belief propagation messagetensors\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.update_all_mts-Tuple{ITensorNetwork, DataGraph}","page":"Home","title":"ITensorNetworks.update_all_mts","text":"Do an update of all message tensors for a given flat ITensornetwork and its partition into sub graphs\n\n\n\n\n\n","category":"method"},{"location":"#ITensorNetworks.update_mt-Tuple{ITensorNetwork, Vector, Vector{ITensors.ITensor}}","page":"Home","title":"ITensorNetworks.update_mt","text":"DO a single update of a message tensor using the current subgraph and the incoming mts\n\n\n\n\n\n","category":"method"},{"location":"#KrylovKit.linsolve","page":"Home","title":"KrylovKit.linsolve","text":"linsolve(\n    A::ITensors.MPO,\n    b::ITensors.MPS,\n    x₀::ITensors.MPS\n) -> ITensors.MPS\nlinsolve(\n    A::ITensors.MPO,\n    b::ITensors.MPS,\n    x₀::ITensors.MPS,\n    a₀::Number\n) -> ITensors.MPS\nlinsolve(\n    A::ITensors.MPO,\n    b::ITensors.MPS,\n    x₀::ITensors.MPS,\n    a₀::Number,\n    a₁::Number;\n    kwargs...\n) -> ITensors.MPS\n\n\nCompute a solution x to the linear system:\n\n(a₀ + a₁ * A)*x = b\n\nusing starting guess x₀. Leaving a₀, a₁ set to their default values solves the  system A*x = b.\n\nTo adjust the balance between accuracy of solution and speed of the algorithm, it is recommed to first try adjusting the solver_tol keyword argument descibed below.\n\nKeyword arguments:\n\nishermitian::Bool=false - should set to true if the MPO A is Hermitian\nsolver_krylovdim::Int=30 - max number of Krylov vectors to build on each solver iteration\nsolver_maxiter::Int=100 - max number outer iterations (restarts) to do in the solver step\nsolver_tol::Float64=1E-14 - tolerance or error goal of the solver\n\nOverload of KrylovKit.linsolve.\n\n\n\n\n\n","category":"function"}]
}
